class DotProductAttention(layers.Layer)
    def __init__(self, dropout_rate=0.1, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout_rate = dropout_rate
    def build(self, input_shape):
        self.dropout = Dropout(self.dropout_rate)
        super(DotProductAttention, self).build(input_shape)
    def call(self, inputs, training=False, mask=None):
        queries, keys, values = inputs
        d = tf.cast(tf.shape(queries)[-1], tf.float32)
        scores = tf.matmul(queries, keys, transpose_b=True)  # [batch_size, query_len, key_len]
        scores = scores / tf.math.sqrt(d)  
        if mask is not None:
            scores += (mask * -1e9)
        attention_weights = tf.nn.softmax(scores, axis=-1)

        attention_weights = self.dropout(attention_weights, training=training)

        output = tf.matmul(attention_weights, values)  # [batch_size, query_len, value_dim]

        return output


class MultiHeadAttentionAdvanced(layers.Layer):
    def __init__(self, num_heads, key_dim, dropout_rate=0.1, use_additive=False, **kwargs):
        super(MultiHeadAttentionAdvanced, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.key_dim = key_dim
        self.dropout_rate = dropout_rate
        self.use_additive = use_additive

    def build(self, input_shape):
        query_shape, key_shape, value_shape = input_shape
        self.model_dim = query_shape[-1]
        assert self.model_dim % self.num_heads == 0"
        self.depth = self.model_dim // self.num_heads
        self.Wq = Dense(self.model_dim, use_bias=False)
        self.Wk = Dense(self.model_dim, use_bias=False)
        self.Wv = Dense(self.model_dim, use_bias=False)
        self.Wo = Dense(self.model_dim, use_bias=False)
        if self.use_additive:
            self.attention = AdditiveAttention(self.model_dim, self.dropout_rate)
        else:
            self.attention = DotProductAttention(self.dropout_rate)

        self.dropout = Dropout(self.dropout_rate)
        super(MultiHeadAttentionAdvanced, self).build(input_shape)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs, training=False, mask=None):
        queries, keys, values = inputs
        batch_size = tf.shape(queries)[0]
        Q = self.Wq(queries)
        K = self.Wk(keys)
        V = self.Wv(values)

        Q = self.split_heads(Q, batch_size)  # [batch_size, num_heads, query_len, depth]
        K = self.split_heads(K, batch_size)  # [batch_size, num_heads, key_len, depth]
        V = self.split_heads(V, batch_size)  # [batch_size, num_heads, value_len, depth]

        Q_reshaped = tf.reshape(Q, (batch_size * self.num_heads, -1, self.depth))
        K_reshaped = tf.reshape(K, (batch_size * self.num_heads, -1, self.depth))
        V_reshaped = tf.reshape(V, (batch_size * self.num_heads, -1, self.depth))

        attention_output = self.attention([Q_reshaped, K_reshaped, V_reshaped], training=training)

        attention_output = tf.reshape(attention_output,
                                      (batch_size, self.num_heads, -1, self.depth))

        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.model_dim))

        output = self.Wo(concat_attention)
        output = self.dropout(output, training=training)

        return output


class LearnablePositionalEncoding(layers.Layer):

    def __init__(self, max_len=100, d_model=128, **kwargs):
        super(LearnablePositionalEncoding, self).__init__(**kwargs)
        self.max_len = max_len
        self.d_model = d_model

    def build(self, input_shape):
        self.position_embedding = self.add_weight(
            name='position_embedding',
            shape=(self.max_len, self.d_model),
            initializer=tf.keras.initializers.RandomNormal(stddev=0.02),
            trainable=True
        )
        super(LearnablePositionalEncoding, self).build(input_shape)

    def call(self, x):
        seq_len = tf.shape(x)[1]
        if seq_len > self.max_len:
            position_encoding = tf.pad(
                self.position_embedding,
                [[0, seq_len - self.max_len], [0, 0]]
            )
        else:
            position_encoding = self.position_embedding[:seq_len, :]

        return x + position_encoding
