class DotProductAttention(layers.Layer):
    def __init__(self, dropout_rate=0.1, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout_rate = dropout_rate
        
    def build(self, input_shape):
        self.dropout = layers.Dropout(self.dropout_rate)
        super(DotProductAttention, self).build(input_shape)
        
    def call(self, inputs, training=False, mask=None):
        queries, keys, values = inputs
        d = tf.cast(tf.shape(queries)[-1], tf.float32)
        scores = tf.matmul(queries, keys, transpose_b=True)  # [batch_size, query_len, key_len]
        scores = scores / tf.math.sqrt(d)
        
        if mask is not None:
            scores += (mask * -1e9)
            
        attention_weights = tf.nn.softmax(scores, axis=-1)
        attention_weights = self.dropout(attention_weights, training=training)
        output = tf.matmul(attention_weights, values)  # [batch_size, query_len, value_dim]
        
        return output


class MultiHeadAttentionAdvanced(layers.Layer):
    def __init__(self, num_heads=8, key_dim=16, dropout_rate=0.1, use_additive=False, **kwargs):
        super(MultiHeadAttentionAdvanced, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.key_dim = key_dim
        self.dropout_rate = dropout_rate
        self.use_additive = use_additive
        
    def build(self, input_shape):
        query_shape, key_shape, value_shape = input_shape
        self.model_dim = query_shape[-1]
        assert self.model_dim % self.num_heads == 0, "model_dim must be divisible by num_heads"
        self.depth = self.model_dim // self.num_heads
        
        kernel_regularizer = regularizers.l2(1e-5)
        
        self.Wq = layers.Dense(self.model_dim, use_bias=False, 
                               kernel_regularizer=kernel_regularizer)
        self.Wk = layers.Dense(self.model_dim, use_bias=False,
                               kernel_regularizer=kernel_regularizer)
        self.Wv = layers.Dense(self.model_dim, use_bias=False,
                               kernel_regularizer=kernel_regularizer)
        self.Wo = layers.Dense(self.model_dim, use_bias=False,
                               kernel_regularizer=kernel_regularizer)
        
        self.attention = DotProductAttention(self.dropout_rate)
        self.dropout = layers.Dropout(self.dropout_rate)
        super(MultiHeadAttentionAdvanced, self).build(input_shape)
        
    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def call(self, inputs, training=False, mask=None):
        queries, keys, values = inputs
        batch_size = tf.shape(queries)[0]
        
        Q = self.Wq(queries)
        K = self.Wk(keys)
        V = self.Wv(values)
        
        Q = self.split_heads(Q, batch_size)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)
        
        Q_reshaped = tf.reshape(Q, (batch_size * self.num_heads, -1, self.depth))
        K_reshaped = tf.reshape(K, (batch_size * self.num_heads, -1, self.depth))
        V_reshaped = tf.reshape(V, (batch_size * self.num_heads, -1, self.depth))
        
        attention_output = self.attention([Q_reshaped, K_reshaped, V_reshaped], 
                                          training=training, mask=mask)
        
        attention_output = tf.reshape(attention_output,
                                      (batch_size, self.num_heads, -1, self.depth))
        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.model_dim))
        
        output = self.Wo(concat_attention)
        output = self.dropout(output, training=training)
        
        return output


class LearnablePositionalEncoding(layers.Layer):
    def __init__(self, max_len=100, d_model=128, **kwargs):
        super(LearnablePositionalEncoding, self).__init__(**kwargs)
        self.max_len = max_len
        self.d_model = d_model
        
    def build(self, input_shape):
        self.position_embedding = self.add_weight(
            name='position_embedding',
            shape=(self.max_len, self.d_model),
            initializer=initializers.RandomNormal(stddev=0.02),
            trainable=True
        )
        super(LearnablePositionalEncoding, self).build(input_shape)
        
    def call(self, x):
        seq_len = tf.shape(x)[1]
        if seq_len > self.max_len:
            position_encoding = tf.pad(
                self.position_embedding,
                [[0, seq_len - self.max_len], [0, 0]]
            )
        else:
            position_encoding = self.position_embedding[:seq_len, :]
        
        return x + position_encoding


class MultiScaleConvFeatureExtraction(layers.Layer):
    def __init__(self, d_model=128, dropout_rate=0.1, **kwargs):
        super(MultiScaleConvFeatureExtraction, self).__init__(**kwargs)
        self.d_model = d_model
        self.dropout_rate = dropout_rate
        
    def build(self, input_shape):
        kernel_regularizer = regularizers.l2(1e-5)
        
        self.conv3 = layers.Conv1D(
            filters=self.d_model, 
            kernel_size=3, 
            padding='same',
            kernel_regularizer=kernel_regularizer
        )
        self.conv5 = layers.Conv1D(
            filters=self.d_model,
            kernel_size=5,
            padding='same',
            kernel_regularizer=kernel_regularizer
        )
        self.conv7 = layers.Conv1D(
            filters=self.d_model,
            kernel_size=7,
            padding='same',
            kernel_regularizer=kernel_regularizer
        )
        
        self.merge = layers.Dense(
            self.d_model,
            kernel_regularizer=kernel_regularizer
        )
        
        self.gelu = layers.Activation('gelu')
        self.dropout = layers.Dropout(self.dropout_rate)
        
        super(MultiScaleConvFeatureExtraction, self).build(input_shape)
        
    def call(self, x, training=False):
        c3 = self.conv3(x)
        c5 = self.conv5(x)
        c7 = self.conv7(x)
        
        c3 = self.gelu(c3)
        c5 = self.gelu(c5)
        c7 = self.gelu(c7)
        
        merged = c3 + c5 + c7 
        merged = self.merge(merged)
        merged = self.gelu(merged)
        merged = self.dropout(merged, training=training)
        
        return merged


class TransformerEncoderBlock(layers.Layer):
    def __init__(self, num_heads=8, d_model=128, dff=512, dropout_rate=0.1, **kwargs):
        super(TransformerEncoderBlock, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.d_model = d_model
        self.dff = dff
        self.dropout_rate = dropout_rate
        
    def build(self, input_shape):
        kernel_regularizer = regularizers.l2(1e-5)
        
        self.pre_norm1 = layers.LayerNormalization(epsilon=1e-6)
        self.pre_norm2 = layers.LayerNormalization(epsilon=1e-6)
        
        self.mha = MultiHeadAttentionAdvanced(
            num_heads=self.num_heads,
            key_dim=self.d_model // self.num_heads,
            dropout_rate=self.dropout_rate
        )
        
        self.residual_weight1 = self.add_weight(
            name='residual_weight1',
            shape=(self.d_model,),
            initializer='ones',
            trainable=True
        )
        self.residual_weight2 = self.add_weight(
            name='residual_weight2',
            shape=(self.d_model,),
            initializer='ones',
            trainable=True
        )
        
        self.ffn = tf.keras.Sequential([
            layers.Dense(self.dff, 
                        kernel_regularizer=kernel_regularizer),
            layers.Activation('gelu'),
            layers.Dropout(self.dropout_rate),
            layers.Dense(self.d_model,
                        kernel_regularizer=kernel_regularizer)
        ])
        
        self.dropout1 = layers.Dropout(self.dropout_rate)
        self.dropout2 = layers.Dropout(self.dropout_rate)
        
        super(TransformerEncoderBlock, self).build(input_shape)
        
    def call(self, x, training=False, mask=None):
        normalized_x = self.pre_norm1(x)
        attn_output = self.mha([normalized_x, normalized_x, normalized_x], 
                              training=training, mask=mask)
        attn_output = self.dropout1(attn_output, training=training)
        
        x = x + self.residual_weight1 * attn_output
        
        normalized_x = self.pre_norm2(x)
        ffn_output = self.ffn(normalized_x)
        ffn_output = self.dropout2(ffn_output, training=training)
        
        x = x + self.residual_weight2 * ffn_output
        
        return x


class TransformerEncoderStack(layers.Layer):
    def __init__(self, num_layers=4, num_heads=8, d_model=128, dff=512, dropout_rate=0.1, **kwargs):
        super(TransformerEncoderStack, self).__init__(**kwargs)
        self.num_layers = num_layers
        self.enc_layers = [
            TransformerEncoderBlock(
                num_heads=num_heads,
                d_model=d_model,
                dff=dff,
                dropout_rate=dropout_rate,
                name=f"transformer_encoder_{i}"
            )
            for i in range(num_layers)
        ]
        
    def call(self, x, training=False, mask=None):
        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training=training, mask=mask)
        return x


class SequenceToVectorModule(layers.Layer):
    def __init__(self, d_model=128, dropout_rate=0.1, **kwargs):
        super(SequenceToVectorModule, self).__init__(**kwargs)
        self.d_model = d_model
        self.dropout_rate = dropout_rate
        
    def build(self, input_shape):
        kernel_regularizer = regularizers.l2(1e-5)
        
        self.attention_pool = layers.Attention(use_scale=True)
        
        self.global_avg_pool = layers.GlobalAveragePooling1D()
        self.global_max_pool = layers.GlobalMaxPooling1D()
        
        self.fusion = tf.keras.Sequential([
            layers.Dense(self.d_model * 2,
                        kernel_regularizer=kernel_regularizer),
            layers.Activation('gelu'),
            layers.Dropout(self.dropout_rate),
            layers.Dense(self.d_model,
                        kernel_regularizer=kernel_regularizer),
            layers.Activation('gelu')
        ])
        
        super(SequenceToVectorModule, self).build(input_shape)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        seq_len = tf.shape(x)[1]
        
        attention_output = self.attention_pool([x, x])
        attention_pooled = tf.reduce_mean(attention_output, axis=1)
        
        avg_pooled = self.global_avg_pool(x)
        
        max_pooled = self.global_max_pool(x)
        
        combined = tf.concat([attention_pooled, avg_pooled, max_pooled], axis=-1)
        fused_features = self.fusion(combined, training=training)
        
        return fused_features

class DeepClassificationHead(layers.Layer):
    def __init__(self, units=[512, 256, 128, 64, 2], dropout_rate=0.1, **kwargs):
        super(DeepClassificationHead, self).__init__(**kwargs)
        self.units = units
        self.dropout_rate = dropout_rate
        
    def build(self, input_shape):
        kernel_regularizer = regularizers.l2(1e-5)
        
        self.dense_layers = []
        self.dropout_layers = []
        
        for i, unit in enumerate(self.units):
            self.dense_layers.append(
                layers.Dense(
                    unit,
                    kernel_regularizer=kernel_regularizer,
                    name=f"dense_{i}"
                )
            )
            if i < len(self.units) - 1: 
                self.dropout_layers.append(
                    layers.Dropout(self.dropout_rate)
                )
        
        self.gelu = layers.Activation('gelu')
        self.softmax = layers.Activation('softmax')
        
        super(DeepClassificationHead, self).build(input_shape)
        
    def call(self, x, training=False):
        for i in range(len(self.dense_layers)):
            x = self.dense_layers[i](x)
            if i < len(self.dense_layers) - 1:  
                x = self.gelu(x)
                if training:  
                    x = self.dropout_layers[i](x, training=training)

        if self.units[-1] > 1:  
            x = self.softmax(x)
        
        return x
